\documentclass[12pt,letterpaper]{article}
\usepackage{fouriernc}
\usepackage{amsmath,amsthm}
\usepackage{mathrsfs}
\usepackage{natbib} % bibliography
\usepackage{bm}

\usepackage{graphicx}
\graphicspath{{./subdiff/}}
\usepackage{epstopdf}

% spacing
\usepackage[headheight=14pt, top=.75in, bottom=.75in, left=.75in, right=.75in]{geometry}
\usepackage{setspace}
\setstretch{1.2}
\usepackage{parskip}

% itemize/enumerate
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}

% links
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0,0,0.3}
\definecolor{darkred}{rgb}{0.3,0,0}
\usepackage[unicode=true,pdfstartview={FitH},bookmarks=false,colorlinks=true,citecolor=darkblue,linkcolor=darkred,urlcolor=darkblue]{hyperref}


\usepackage{stats-v3}
\theoremstyle{definition}\newtheorem*{conject}{Conjecture}


% headers and footers
\usepackage{fancyhdr}
\usepackage{lastpage}
\fancypagestyle{firstpage} {
\fancyhf{}
\setlength{\headheight}{15pt}
\lhead{Yun Ling, Martin Lysy}
\rhead{\today}
\rfoot{{\small \textsl{\thepage/\pageref{LastPage}}}}
}
\fancyhf{}
\lhead{\textsl{Detailed R Report}}
\rfoot{{\small \textsl{\thepage/\pageref{LastPage}}}}

\pagestyle{fancy}

% title & abstract
\usepackage{titling}
\pretitle{\begin{center}\scshape\Large}
\posttitle{\par\end{center}}
\setlength{\droptitle}{-2em}
\usepackage[runin]{abstract}
\abslabeldelim{.}
\setlength{\abstitleskip}{-\absparindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\dft}{\text{drift}}
\newcommand{\fbm}{\text{fBM}}
\newcommand{\toep}{\text{Toeplitz}}
\newcommand{\msd}{\text{MSD}}
\newcommand{\acf}{\text{Acf}}
\newcommand{\dt}{\Delta t}
\newcommand{\hd}{\widehat{D}}	
\newcommand{\ha}{\widehat{H}}
\newcommand{\dx}{\Delta X}
\newcommand{\dy}{\Delta Y}
\newcommand{\p}{\mathcal{P}}
\newcommand{\eps}{\varepsilon}
\begin{document}
<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
if(FALSE) {
  knit2pdf(input = "SuperGauss.Rnw")
}
@

\title{Detailed R Report}
\date{}

\maketitle
\thispagestyle{firstpage}
\vspace{-4em}

\section{Model Description}
Our model consists of trajectory model and camera error model. Trajectory model describes the movement of the mucus, while the camera error models recover the actual movement of mucus from the observation by interpreting the behavior of camera.

\subsection{Trajectory Model}
In our theory, trajectories $Z_t$ follow generalized Brownian Motion with linear drift.
\begin{align*}
	Z(t) = \mu t + \Sigma^{1/2} B_\alpha(t)
\end{align*}
where $\mu =  \left[\mu_1, \mu_2\right]$ is coordinate-wise linear drift, $\Sigma = \begin{bmatrix}
\Sigma_{1,1} & \Sigma_{1,2} \\
\Sigma_{1,2} & \Sigma_{2,2}
\end{bmatrix}$ is variance matrix in 2-d plane. $Z(t) = \left[ Z_1(t), Z_2(t) \right]$ is the 2-d particle trajectory, $B_\alpha(t) = \left[B_1(t), B_2(t)\right]$ is a 2-d fBM pocess with parameter $\alpha$, which determines the degree of autocorrelation of the whole process.\\
In real application, observation are recorded evenly, which makes our observed trajectories discrete process:
\begin{align*}
	& Z_n = n \mu  \dt + \Sigma^{1/2} B_n, \quad n = 1,2,\cdots, N \\
	& Z_n = Z(n\dt) \quad B_n = B_\alpha(n\dt)
\end{align*}
In this report, we are looking into the observation of some water molecule at frequency of $\dt = \dfrac{1}{60}$. If the observation is the true position of molecule, fitted $\widehat{\alpha}$ should be around 1, since water molecule follows standard Brownian motion, which is a special case of fBM process ($\alpha = 1$ ). However, our observation shows sigfinicant positive autocorrelation and our estimation is far above 1. Thus we develops three camera error model in order to explain this phenomenon.

\subsection{Camera Error Model: Lag-1 Autoregression Model}
This model says that while camera interprets the particle position it uses the information of previous observation. Knowing that particles cannot travel too far away from previous spot, camera puts more weight on the observation that are closer to previous location like this:
Observed position is a weighted average between the last-time observation and real position. 
\begin{align*}
	Y_n &= \rho Y_{n-1} + (1-\rho)Z_n 
\end{align*}
where $\rho$ is the weight parameter, $0 \leq \rho \leq 1$. $Z_t$ is the real position of molecule, $Y_t$ is the 2-d observation.

\subsection{Camera Error Model: Lag-1 Moving Average Model}
Motivation of this model is very similar to AR(1) model. This model uses real postion at time n-1 $Z_{n-1}$ instead of observation at time n-1 $Y_{n-1}$. This model is less persuasive than AR(1) model, since it is hard to believe that camera keeps the record of the density cloud of $Z_{n-1}$.\\
Observed position is a weighted average between the last-time position and real position in this model:
\begin{align*}
	Y_n &= \rho Z_{n-1} + (1-\rho) Z_n 
\end{align*}
where $\rho$ is the weight parameter, $0 \leq \rho \leq 1$. $Z_t$ is the real position of molecule, $Y_t$ is the 2-d observation.

\subsection{Camera Error Model: Dynamic Error Model}
Instead of trying to understand how camera inteprets the data, this model focuses on how camera captures data. In phtotgraphy in order to obtain a picture, camera shutter will open for a certain amount time so that electronic sensor can capture the light. This means that camera actually records a average displacement of particle over the time period $\tau$ that the camera shutter is open. Based on this idea we develop such dynamic camera error model.
\begin{align*}
	Y_n&= \int_{0}^{\tau}Z(n\dt-s) ds
\end{align*}
where $\tau$ is the length of exposure, $0 < \tau < \dt$. When $\tau$ approachs 0, $Y_n$ converges to $Z(n\dt)$. When $\tau$ increases, $Y_n$ becomes an average of longer path of $Z(t)$ and will show more ``positive autocorrelation''.

\section{Inference With Profile Likelihood}
Our package ``SuperGauss'' and ``lmn'' returns the profile likelihood of linear matrix models in a single function \textit{lmn.prof}. \\
For matrix normal distribution $Y \sim MN_{n\times d}(X \beta, V, \Sigma)$, its log-likelihood is
\begin{align*}
	\ell(\beta, V, \Sigma | X, Y) = -\dfrac{d}{2} \log|V| - \dfrac{n}{2} \log|\Sigma| - \dfrac{1}{2} \text{tr} \left[ \Sigma^{-1} (Y - X \beta)^T V^{-1} (Y - X \beta) \right]
\end{align*}
When we treat $V$ as given
\begin{align*}
	& \dfrac{\partial \ell(\beta, \Sigma | X, Y, V)}{\partial \beta} = 0 \rightarrow \widehat{\beta} = \left(X^T V^{-1} X\right)^{-1}X^T V^{-1} Y \\
	& \dfrac{\partial \ell(\beta, \Sigma | X, Y, V)}{\partial \Sigma} = 0 \rightarrow \widehat{\Sigma} = \left(Y - X\widehat{\beta} \right)^T V^{-1} \left(Y - X\widehat{\beta} \right)
\end{align*}
we can find that both $\widehat{\beta}$ and $\widehat{\Sigma}$ are functions of $V$. By applying $\widehat{\beta}$ and $\widehat{\Sigma}$ in log-likelihood
\begin{align*}
	\ell(V | X, Y) = -\dfrac{d}{2} \log|V| - \dfrac{n}{2} \log|\left(Y - X\widehat{\beta} \right)^T V^{-1} \left(Y - X\widehat{\beta} \right)|
\end{align*}
which is the return value of \textit{lmn.prof} with argument $\{V, X, Y\}$ only. Then optimization of  log-likelihood becomes 1-d optimization. With estimated $\widehat{V}$ we can then compute $\widehat{\beta}$ and $\widehat{\Sigma}$ using function \textit{lmn.suff}. Here are some demonstration.

\subsection{Basic Trajectory Model}
In order to obtain stationary Gaussian process, our inference is applied on the increment of $Z_t$
\begin{align*}
& \Delta Z_n = \mu \dt + \Sigma^{1/2}  \Delta B_n \\
& 
\end{align*}
where $\Delta Z_t = Z_1$. By doing this, $\Delta Z_t$ is a Continuous Stationary Gaussian that follows matrix normal distribution $MN(\mu \dt, \Sigma, V_\alpha)$, where $V_\alpha$, determined by $\alpha$, is the variance matrix of $\Delta B_t$. Since mean parameter $\mu$ and variance parameter $\Sigma$ are ``free parameters'' in profile likelihood estimation, our optimization is a 1-d optimization over $\alpha$.\\
Optimization consists of three steps: \\
Write down the profile likelihood function, where acf of $V_\alpha$ is given by function \textit{fbm.acf}.
<<proflike-of-fbm1, eval=FALSE, dev='png'>>=
fbm.loglik <- function(alpha, dX, dT, toep){
  N <- nrow(dX)
  acf <- fbm.acf(alpha, dT, N)
  toep$setAcf(acf)
  lmn.prof(Y = dX, X = dT, acf = toep, noSigma = FALSE)
}
@
Use R function ``optim'' to find the $\widehat{\alpha}$ that maximize the likelihood.
<<proflike-of-fbm3, eval=FALSE, dev='png'>>=
fbm.mle <- function(dX, dT, toep){
  ans <- optimize(f = fbm.loglik, dX = dX, dT = dT, toep = toep,
                  interval = c(0, 2), maximum = TRUE)$maximum
}
@
Compute $\mu$ and $\Sigma$ with $\widehat{\alpha}$.
<<proflike-of-fbm4, eval=FALSE, dev='png'>>=
prof.get.fbm <- function(dX, dT, alpha, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  acf1 <- fbm.acf(alpha = alpha, dT = dT, N = N)
  toep$setAcf(acf1)
  suff <- lmn.suff(Y = dX, X = dT, acf = toep)
  Sigma <- suff$S / suff$n
  mu <- suff$Beta.hat
  list(Sigma = Sigma, mu = mu)
}
@
Here is the estimation for all paths:
<<proflike-of-fbm5, eval=FALSE, dev='png'>>=
## assign the storage
alpha <- rep(NA, npath)
tau <- rep(NA, npath)
rho <- rep(NA, npath)
SigmaMat <- matrix(NA, 2*npath, 2)
Mu <- matrix(NA, npath, 2)
## computation
system.time({
  for(ii in 1:npath){
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    toep <- Toeplitz(nrow(dX))
    fbm.ans <- fbm.mle(dX, dT, toep)
    alpha[ii] <- fbm.ans
    suff <- prof.get.fbm(dX, dT, fbm.ans, toep)
    SigmaMat[(-1:0)+(2*ii), ] <- suff$Sigma
    Mu[ii, ] <- suff$mu
  }
})
@
\subsection{AR(1) Model}
Our inference of AR(1) model is also based on the increment form:
\begin{align*}
	\Delta Y_n = \rho \Delta Y_{n-1} + \Delta Z_n
\end{align*}
In AR(1) model, we can recover $\Delta Z_n$ with given $\rho$ by 
\begin{align*}
	\Delta Z_n = \Delta Y_n - \rho \Delta Y_{n-1}
\end{align*}
Thus we can compute the profile likelihood of AR(1) model by
<<proflike-of-ar1, eval=FALSE, dev='png'>>=
ar.loglik <- function(theta, dX, dT, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  alpha <- theta[1]
  rho <- theta[2]
  dX <- dX - rho * rbind(dX[-1, ], rep(0, d))
  acf <- fbm.acf(alpha, dT, N)
  toep$setAcf(acf)
  lmn.prof(Y = dX, X = dT, acf = toep, noSigma = FALSE)
}
@
Following steps are quite similar to fBM case
<<proflike-of-ar2, eval=FALSE, dev='png'>>=
ar.mle <- function(dX, dT, toep){
  ans <- optim(par = c(1, 1), fn = ar.loglik, dX = dX, dT = dT, toep = toep, control = list(fnscale = -1))$par
}
prof.get.ar <- function(dX, dT, alpha, rho, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  dX <- dX - rho * rbind(dX[-1, ], rep(0, d))
  acf <- fbm.acf(alpha, dT, N)
  toep$setAcf(acf)
  suff <- lmn.suff(Y = dX, X = dT, acf = toep)
  Sigma <- suff$S / suff$n
  mu <- suff$Beta.hat
  list(Sigma = Sigma, mu = mu)
}
@
And here are the estimation for all paths
<<proflike-of-ar3, eval=FALSE, dev='png'>>=
## assign the storage
alpha <- rep(NA, npath)
rho <- rep(NA, npath)
SigmaMat <- matrix(NA, 2*npath, 2)
Mu <- matrix(NA, npath, 2)
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    toep <- Toeplitz(nrow(dX))
    ar.ans <- ar.mle(dX, dT, toep)
    alpha[ii] <- ar.ans[1]
    rho[ii] <- ar.ans[2]
    suff <- prof.get.ar(dX, dT, dyn.ans[1], dyn.ans[2], toep)
    SigmaMat[(-1:0)+(2*ii), ] <- suff$Sigma
    Mu[ii, ] <- suff$mu
  }
})
@
\subsection{MA(1) Model}
Inference of MA(1) model is similar to that of AR(1) model. 
\begin{align*}
	\Delta Y_n = \rho \Delta Z_n + \Delta Z_{n-1}
\end{align*}
We can also recover $\Delta Z_n$ with given $\rho$ by
\begin{align*}
	\Delta Z_1 &= \Delta Y_1 \\
	\Delta Z_n &= \Delta Y_n - (1-\rho)\Delta Z_{n-1}, n = 2, \cdots
\end{align*}
Thus we can compute the profile likelihood of AR(1) model by
<<proflike-of-ma, eval=FALSE, dev='png'>>=
ma.loglik <- function(theta, dX, dT, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  alpha <- theta[1]
  rho <- theta[2]
  for(ii in 2:N){
    dX[ii,] <- dX[ii,] - rho * dX[ii - 1,]
  }
  acf1 <- fbm.acf(alpha, dT, N)
  toep$setAcf(acf1)
  lmn.prof(Y = dX, X = dT, acf = toep, noSigma = FALSE)
}
@
And followed by
<<proflike-of-ma1, eval=FALSE, dev='png'>>=
ma.mle <- function(dX, dT, toep){
  ans <- optim(par = c(1, 1), fn = ma.loglik, dX = dX, dT = dT, toep = toep, control = list(fnscale = -1))$par
}

prof.get.ma <- function(dX, dT, alpha, rho, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  for(ii in 2:N){
    dX[ii,] <- dX[ii,] - rho * dX[ii - 1,]
  }
  acf1 <- fbm.acf(alpha = alpha, dT = dT, N = N)
  toep$setAcf(acf1)
  suff <- lmn.suff(Y = dX, X = dT, acf = toep)
  Sigma <- suff$S / suff$n
  mu <- suff$Beta.hat
  list(Sigma = Sigma, mu = mu)
}
@
And here are the estimation for all paths
<<proflike-of-ma2, eval=FALSE, dev='png'>>=
## assign the storage
alpha <- rep(NA, npath)
rho <- rep(NA, npath)
SigmaMat <- matrix(NA, 2*npath, 2)
Mu <- matrix(NA, npath, 2)
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    toep <- Toeplitz(nrow(dX))
    ma.ans <- ma.mle(dX, dT, toep)
    alpha[ii] <- ma.ans[1]
    rho[ii] <- ma.ans[2]
    suff <- prof.get.ma(dX, dT, dyn.ans[1], dyn.ans[2], toep)
    SigmaMat[(-1:0)+(2*ii), ] <- suff$Sigma
    Mu[ii, ] <- suff$mu 
  }
})
@

\subsection{Dynamic Error Model}
For the increment form of dynamic error model
\begin{align*}
	\Delta Y_n = \int_{0}^{\tau} \left(Z_{n\dt - s} - Z_{(n-1)\dt - s}\right)ds
\end{align*}
and $\Delta Y_n$ is also a Continuous Stationary Gaussian process following $MN(\mu\dt\tau, \Sigma, T_{\alpha, \tau})$, acf of $T_{\alpha, \tau}$ is given by function \textit{fdyn.acf}. It is a 2-d optimization over parameter $\{\alpha, \tau\}$ with such profile likelihood:
<<proflike-of-dyn, eval=FALSE, dev='png'>>=
dyn.loglike <- function(theta, dX, dT, toep){
  N <- nrow(dX)
  alpha <- theta[1]
  tau <- theta[2] * dT
  acf1 <- fdyn.acf(alpha, tau, dT, N)
  toep$setAcf(acf1)
  lmn.prof(Y = dX, X = dT, acf = toep, noSigma = FALSE)
}
dyn.mle <- function(dX, dT, toep){
  ans <- optim(par = c(1, .5), fn = dyn.loglike, dX = dX, dT = dT, toep = toep, control = list(fnscale = -1))$par
  ans
}
prof.get.dyn <- function(dX, dT, alpha, tau, toep){
  N <- nrow(dX)
  d <- ncol(dX)
  acf1 <- fdyn.acf(alpha = alpha, sigma = tau*dT, dT = dT, N = N)
  toep$setAcf(acf1)
  suff <- lmn.suff(Y = dX, X = dT, acf = toep)
  Sigma <- suff$S / suff$n
  mu <- suff$Beta.hat
  list(Sigma = Sigma, mu = mu)
}
@
And here are the estimation for all paths
<<proflike-of-dyn1, eval=FALSE, dev='png'>>=
## assign the storage
alpha <- rep(NA, npath)
tau <- rep(NA, npath)
SigmaMat <- matrix(NA, 2*npath, 2)
Mu <- matrix(NA, npath, 2)
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    toep <- Toeplitz(nrow(dX))
    dyn.ans <- dyn.mle(dX, dT, toep)
    alpha[ii] <- dyn.ans[1]
    tau[ii] <- dyn.ans[2]
    suff <- prof.get.dyn(dX, dT, dyn.ans[1], dyn.ans[2], toep)
    SigmaMat[(-1:0)+(2*ii), ] <- suff$Sigma
    Mu[ii, ] <- suff$mu
  }
})
@
Here is the comparison of estimated $\alpha$ in four different model:

\section{Simulation}
One way to validate our models is to simulate paticle movement using path-wise estimated parameters. Conclusions can be draw by comparing the original data and simulated data.

\subsection{Simulation of fBM}
Easiest way of simulating fBM process is to first simulate a series of fractional Gaussian noise and then take cumulative sums. Here is the simulation process:
<<simulate-of-fbm3, eval=FALSE, dev='png'>>=
fbmSim <- function(alpha, Sigma, mu, dT, N){
  acf <- fbm.acf(alpha, dT, N)
  Xt <- rSnorm(n = 2, acf = acf)
  Xt <- apply(Xt, 2, cumsum) 
  Xt <- Xt %*% chol(Sigma) + t(matrix(dT, 2, N) * mu)
  Xt
}
## assign the storage
fbm.sim <- paths
alphaMat <- alpha.data$fbm
SigmaMat <- Sigma.data$fbm
Mu <- mu.data$fbm
## simulation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    alpha <- alphaMat[ii]
    Sigma <- sigma.read(SigmaMat, ii)
    mu <- Mu[ii, ]
    Xt <- as.matrix(fbm.sim[fbm.sim$ID == ii,-1])
    N <- nrow(Xt)
    Xt <- fbmSim(alpha, Sigma, mu, dT, N)
    fbm.sim[fbm.sim$ID == ii,-1] <- Xt
  }
})
@

\subsection{Simulation of AR(1) model}
Easiest way of simulating fBM process is to first simulate a series of fractional Gaussian noise and then take cumulative sums. Here is the simulation process:
<<simulate-of-ar, eval=FALSE, dev='png'>>=
arSim <- function(alpha, rho, Sigma, mu, dT, N){
  acf <- fbm.acf(alpha, dT, N)
  Xt <- rSnorm(n = 2, acf = acf)
  Xt <- apply(Xt, 2, cumsum)
  Yt <- Xt
  for(ii in 2:N){
    Yt[ii, ] <- rho * Yt[ii-1, ] + Xt[ii, ]
  }
  Yt <- Yt %*% chol(Sigma) + t(matrix(dT, 2, N) * mu)
  Yt
}
## assign the storage
ar.sim <- paths
alphaMat <- alpha.data$ar
tauMat <- tau.data$ar
SigmaMat <- Sigma.data$ar
Mu <- mu.data$ar
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    alpha <- alphaMat[ii]
    rho <- tauMat[ii]
    Sigma <- sigma.read(SigmaMat, ii)
    mu <- Mu[ii, ]
    Xt <- as.matrix(ar.sim[ar.sim$ID == ii,-1])
    N <- nrow(Xt)
    Xt <- arSim(alpha, rho, Sigma, mu, dT, N)
    ar.sim[ar.sim$ID == ii,-1] <- Xt
  }
})
@

\subsection{Simulation of MA(1) model}
Easiest way of simulating fBM process is to first simulate a series of fractional Gaussian noise and then take cumulative sums. Here is the simulation process:
<<simulate-of-fbm1, eval=FALSE, dev='png'>>=
maSim <- function(alpha, rho, Sigma, mu, dT, N){
  acf <- fbm.acf(alpha, dT, N)
  Xt <- rSnorm(n = 2, acf = acf)
  Xt <- apply(Xt, 2, cumsum)
  Yt <- rho * Xt
  for(ii in 2:N){
    Yt[ii, ] <- Yt[ii, ] + Xt[ii-1, ]
  }
  Yt <- Yt %*% chol(Sigma) + t(matrix(dT, 2, N) * mu)
  Yt
}
## assign the storage
ma.sim <- paths
alphaMat <- alpha.data$ma
tauMat <- tau.data$ma
SigmaMat <- Sigma.data$ma
Mu <- mu.data$ma
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    alpha <- alphaMat[ii]
    rho <- tauMat[ii]
    Sigma <- sigma.read(SigmaMat, ii)
    mu <- Mu[ii, ]
    Xt <- as.matrix(ma.sim[ma.sim$ID == ii,-1])
    N <- nrow(Xt)
    Xt <- maSim(alpha, rho, Sigma, mu, dT, N)
    ma.sim[ma.sim$ID == ii,-1] <- Xt
  }
})
@

\subsection{Simulation of Dynamic Error Model}
Easiest way of simulating fBM process is to first simulate a series of fractional Gaussian noise and then take cumulative sums. Here is the simulation process:
<<simulate-of-fbm2, eval=FALSE, dev='png'>>=
dynSim <- function(alpha, tau, Sigma, mu, dT, N){
  acf <- fdyn.acf(alpha, tau * dT, dT, N)
  Xt <- rSnorm(n = 2, acf = acf)
  Xt <- apply(Xt, 2, cumsum)
  Xt <- Xt %*% chol(Sigma) + t(matrix(dT, 2, N) * mu)
  Xt
}
## assign the storage
dyn.sim <- paths
alphaMat <- alpha.data$dyn
tauMat <- tau.data$dyn
SigmaMat <- Sigma.data$dyn
Mu <- mu.data$dyn
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    alpha <- alphaMat[ii]
    tau <- tauMat[ii]
    Sigma <- sigma.read(SigmaMat, ii)
    mu <- Mu[ii, ]
    if(tau < 0.1){
      dyn.sim[dyn.sim$ID == ii,-1] <- 0
    }else{
      Xt <- as.matrix(dyn.sim[dyn.sim$ID == ii,-1])
      N <- nrow(Xt)
      Xt <- dynSim(alpha, tau, Sigma, mu, dT, N)
      dyn.sim[dyn.sim$ID == ii,-1] <- Xt
    }
  }
})
@

\section{Model Validation}

\subsection{Empirical MSD Estimate}
In this part we are goint to estimate $\alpha$ parameter from the empirical MSD.\\
For a 2-d movement $X(t) = \{X_1(t), X_2(t)\}$, its MSD is defined as
\begin{align*}
	\text{MSD}(t) = E[X(t) - X(0)]^2 = E[X_1(t) - X_1(0)]^2 + E[X_2(t) - X_2(0)]^2
\end{align*}
And for fBM processes, its MSD is defined by $D$ and $\alpha$:
\[
\text{MSD}(t) = D t^\alpha
\]
Thus we can estimate $\alpha$ in a model-free way by fitting
\[
\log \text{MSD}_t = \log D + \widehat{\alpha} \log t
\]
Here is the function \textit{msd} that returns the MSD of a time series and function \textit{msd2alpha} that returns the estimated $\alpha$ from MSD
<<MSD, eval=FALSE, dev='png'>>=
## compute the MSD of time series Xt
msd <- function(Xt, length = 30){
  d <- ncol(Xt)
  eta <- rep(0, length)
  for(ii in 1:length){
    for(jj in 1:d){
      eta[ii] <- eta[ii] + mean(diff(Xt[, jj], lag = ii)^2)
    }
  }
  eta
}
## convert MSD into the alpha estimation
msd2alpha <- function(eta){
  N <- length(eta)
  tSeq <- 1:N
  alpha <- lm(log(eta)~log(tSeq))$coef[2]
  as.numeric(alpha)
}
@
Here are procedures that computes the estimated $\alpha$ from MSD of simulation and its downsampling data.\\
For original data:
<<alpha-est, eval=FALSE, dev='png'>>=
## assign the storage
alphaMSD <- rep(NA, npath)
alphaMSD2 <- rep(NA, npath)
alphaMSD3 <- rep(NA, npath)
alphaMSD4 <- rep(NA, npath)
## computation
system.time({
  for(ii in 1:npath){
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX2 <- downSample(Xt, step = 2)
    dX3 <- downSample(Xt, step = 3)
    dX4 <- downSample(Xt, step = 4)
    alphaMSD[ii] <- msd2alpha(msd(Xt, length))
    alphaMSD2[ii] <- msd2alpha(msd(dX2, length))
    alphaMSD3[ii] <- msd2alpha(msd(dX3, length))
    alphaMSD4[ii] <- msd2alpha(msd(dX4, length))
  }
})
@
Here are the model-free estimation of $\alpha$ for simulated data:\\
Well, the results look really strange...
\subsection{identifiability of $\rho$ and $\tau$}

By comparing the estimation of $\tau$ with and without the knowledge that $\alpha = 1$ in dynamic error model, we can have some idea of the identifiability of $\rho$ in dynamic error model. Here is the estimation of $\tau$ given that $\alpha$

<<iden-dyn, eval=FALSE, dev='png'>>=
## profile likelihood of dynamic model, alpha = 1
dyn.loglik.test <- function(tau, dX, dT, toep){
  N <- nrow(dX)
  acf1 <- fdyn.acf(1, tau * dT, dT, N)
  toep$setAcf(acf1)
  lmn.prof(Y = dX, X = dT, acf = toep, noSigma = FALSE)
}
## optimization over tau
dyn.mle.test <- function(dX, dT, toep){
  ans <- optimize(f = dyn.loglik.test, dX = dX, dT = dT, toep = toep, interval = c(0, 1), 
                  maximum = TRUE)$maximum
  ans
}
## assign the storage
tau <- rep(NA, npath)
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    toep <- Toeplitz(nrow(dX))
    tau[ii] <- dyn.mle.test(dX, dT, toep)
  }
})
@
Here is the estimation results:\\
We also use this method to check the identifiability of $\rho$ parameter in AR(1) and MA(1) model, and here is the result:\\
\subsection{Model Residules}
One way to check whether $X_t$ follows estimated distribution
\begin{align*}
	X_t \sim MN(\mu, \Sigma, V)
\end{align*}
is to transform $X_t$ into the combination of some i.i.d. multivariate normal process. We have that
\begin{align*}
	 & A^{-1} \left(X_t - \mu\right) B^{-1} \sim MN(0, I_{d \times d}, I_{N \times N}) \\
	 & AA^T = V, \quad B^TB = \Sigma
\end{align*}
Considering that $X_t$ is a linear time series, we use Choleski decomposition to obtain $A$, so that  $A$ is in the form of a lower-triangular matrix. By doing this $A X_t$ can be viewed as the time-ordered linear combination of time series $X_t$. On the other size, $\Sigma$ as a variance matrix that works as an permutation parameter in matrix Gaussian process, its square root should come from orthogonal decimposition, so that $A^{-1} \left(X_t - \mu\right) B^{-1}$ can be still interpreted as the convolution of two time series.\\
Here is the function that ``normalize'' $X_t$:
<<normalize, eval=FALSE, dev='png'>>=
ToepNormal <- function(dX, acf, Sigma, mu, dT){
  N <- nrow(dX)
  d <- ncol(dX)
  dX <- dX - t(matrix(dT, d, N) * mu)
  dX <- cholXZ(dX, acf)
  a1 <- svd(Sigma)
  InvSig <- a1$u %*% diag(sqrt(1/a1$d))
  dX <- dX %*% InvSig
  ad.test(dX[,1])$p.value
}
@
In order to check whether $A^{-1} \left(X_t - \mu\right) B^{-1}$ follows standard matrix Gaussian distribution, we need to test whether its first column follows standard Gaussian process using Anderson-Darling test. Here is the code for computing the p-value of the fBM simulation data:
<<normalize1, eval=FALSE, dev='png'>>=
## assign the storage
pvalue.fbm <- rep(NA, npath)
alphaMat <- alpha.data$fbm
SigmaMat <- Sigma.data$fbm
Mu <- mu.data$fbm
## computation
system.time({
  for(ii in 1:npath){
    if(!(ii %% 100)){
      print(ii)
    }
    alpha <- alphaMat[ii]
    Sigma <- sigma.read(SigmaMat, ii)
    mu <- Mu[ii, ]
    Xt <- as.matrix(paths[paths$ID == ii,-1])
    dX <- as.matrix(apply(Xt, 2, diff))
    acf1 <- fbm.acf(alpha, dT, nrow(dX))
    pvalue.fbm[ii] <- ToepNormal(dX, acf1, Sigma, mu, dT)
  }
})
@
Process for computing the p-value for remaining models are very similar. \\
Here is the summary of p-value of four different models:\\
And following is the proportion of paths that p-value is greater than 5\%:
\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
  & fBM & dynamic & AR(1) & MA(1) \\
  \hline
 p-value & 0.723 & 0.722 & 0.729 & 0.882 \\ 
 \hline
\end{tabular}
\end{center}
\end{document}
